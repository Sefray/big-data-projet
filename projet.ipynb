{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student 1 : Philippe BERNET  23130\n",
    "\n",
    "Student 2 : Marius   DUBOSC  23527\n",
    "\n",
    "Student 3 : Baptiste BOURDET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import when, count, col, hour, mean, year, window, udf, avg, month, min, max, dayofmonth, weekofyear, lag, isnull\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"stocks_data/\"\n",
    "companies = ['AMAZON',\n",
    "             'APPLE',\n",
    "             'FACEBOOK',\n",
    "             'GOOGLE',\n",
    "             'MICROSOFT',\n",
    "             'TESLA',\n",
    "             'ZOOM']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_application_name = \"Spark_Application_Project\"\n",
    "spark = (SparkSession.builder.appName(spark_application_name).getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCol = StructType([StructField(\"Date\",TimestampType()), StructField(\"High\",DoubleType()), StructField(\"Low\",DoubleType()), StructField(\"Open\",DoubleType()), StructField(\"Close\",DoubleType()), StructField(\"Volume\", DoubleType()), StructField(\"Adj Close\", DoubleType()), StructField(\"company_name\",StringType())])\n",
    "dfSchema = StructType(dfCol)\n",
    "\n",
    "df = spark.read.csv(data_folder, header=True, sep=',', schema=dfSchema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function allow you to explore the dataset. A widget is available to facilitate the visualisation and allow to dynamically change the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_ds(df, input):\n",
    "\tdf_one = df.filter(df.company_name == input).orderBy('Date')\n",
    "\n",
    "\t# Show the first and last 40 rows of stock price\n",
    "\tprint('First 40 lines :')\n",
    "\tspark.createDataFrame(df_one.head(40)).show(40)\n",
    "\tprint('Last 40 lines :')\n",
    "\tspark.createDataFrame(df_one.tail(40)).show(40)\n",
    "\n",
    "\t# Get the number of observations\n",
    "\tprint(f'Number of observations : {df_one.count()}')\n",
    "\n",
    "\t# Period\n",
    "\tdini = df_one.select(min('Date')).collect()[0][0]\n",
    "\tdfin = df_one.select(max('Date')).collect()[0][0]\n",
    "\tinter = (dfin - dini).days\n",
    "\tprint(f'Period between {dini} and {dfin} : {inter} days')\n",
    "\n",
    "\t# Descriptive statistics\n",
    "\tprint('Statistics :')\n",
    "\tdf_one.summary().show()\n",
    "\n",
    "\t# Number of missing values\n",
    "\tprint('Missing values :')\n",
    "\tdf_one.select([count(when(col(c).isNull(), c)).alias(c) for c in df_one.columns]).show()\n",
    "\n",
    "\t# Correlation between values\n",
    "\tcols = [\"High\", \"Low\", \"Open\", \"Close\", \"Volume\", \"Adj Close\"]\n",
    "\tfor i in range(len(cols)):\n",
    "\t\tfor j in range(i + 1, len(cols)):\n",
    "\t\t\tprint(f\"Correlation {cols[i]}/{cols[j]}: {df_one.corr(cols[i], cols[j])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_info_by_company(df):\n",
    "    w_output = widgets.Output()\n",
    "    w_inputs = widgets.Dropdown(\n",
    "        options=[(companies[i], i) for i in range(len(companies))],\n",
    "        value=0,\n",
    "        description=\"Input\",\n",
    "    )\n",
    "\n",
    "    def intern_display(change):\n",
    "        w_output.clear_output()\n",
    "        with w_output:\n",
    "            input = w_inputs.options[w_inputs.value][0]\n",
    "            explore_ds(df, input)\n",
    "        \n",
    "    w_inputs.observe(intern_display)\n",
    "    display(w_inputs)\n",
    "    display(w_output)\n",
    "\n",
    "display_info_by_company(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to change date granularity\n",
    "granularity = {\n",
    "    'day' : ['company_name', year('Date').alias('year'), month('Date').alias('month'),  dayofmonth('Date').alias('day')],\n",
    "    'week' : ['company_name', year('Date').alias('year'), weekofyear('Date').alias('week')],\n",
    "    'month' : ['company_name', year('Date').alias('year'), month('Date').alias('month')],\n",
    "    'year' : ['company_name', year('Date').alias('year')]\n",
    "}\n",
    "\n",
    "# used to visualize the data in chronological order\n",
    "order_by = {\n",
    "\t'day' : ['year', 'month', 'day'],\n",
    "\t'week' : ['year', 'week'],\n",
    "\t'month' : ['year', 'month'],\n",
    "\t'year' : ['year'],\n",
    "}\n",
    "\n",
    "columns = ['Close', 'Open', 'High', 'Low', 'Volume', 'Adj Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the exploration by computing the average values for the open and close price as well as the volume for dfferent time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average(cur_df, gran = 'year', cols = (mean('Open').alias('Open_Mean'), mean('Close').alias('Close_mean'), mean('Volume').alias('Volume_mean'))):\n",
    "    return (cur_df\n",
    "    .groupBy(granularity[gran])\n",
    "    .agg(*cols).orderBy(order_by[gran]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the variation of the average values of the close price and the volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variation(cur_df, gran = 'year', cols = (mean('Close').alias('Close_mean'), mean('Volume').alias('Volume_mean'))):\n",
    "    cur_df = compute_average(df, gran, cols)\n",
    "    my_window = Window.partitionBy(\"company_name\").orderBy(order_by[gran])\n",
    "    cur_df = cur_df.withColumn(\"prev_close\", lag(cur_df.Close_mean).over(my_window))\n",
    "    cur_df = cur_df.withColumn(\"Close_diff\", when(isnull(cur_df.Close_mean - cur_df.prev_close), 0)\n",
    "                            .otherwise(cur_df.Close_mean - cur_df.prev_close))\n",
    "    cur_df = cur_df.drop(\"prev_close\")\n",
    "\n",
    "    cur_df = cur_df.withColumn(\"prev_volume\", lag(cur_df.Volume_mean).over(my_window))\n",
    "    cur_df = cur_df.withColumn(\"Volume_diff\", when(isnull(cur_df.Volume_mean - cur_df.prev_volume), 0)\n",
    "                                .otherwise(cur_df.Volume_mean - cur_df.prev_volume))\n",
    "    cur_df = cur_df.drop(\"prev_volume\")\n",
    "    return cur_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a generic function enabling dynamic visualisation with widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(func, df, cols = (mean('Open').alias('Open_Mean'), mean('Close').alias('Close_mean'), mean('Volume').alias('Volume_mean'))):\n",
    "    w_granu = widgets.RadioButtons(\n",
    "        options=granularity.keys(),\n",
    "        value=list(granularity.keys())[0],\n",
    "        description=\"Granularity\",\n",
    "    )\n",
    "\n",
    "    w_output_g = widgets.Output()\n",
    "\n",
    "    def f(change):\n",
    "        w_output_g.clear_output()\n",
    "        with w_output_g :\n",
    "            func(df, w_granu.value, cols).show()\n",
    "                \n",
    "\n",
    "    w_granu.observe(f)\n",
    "    display(w_granu)\n",
    "    display(w_output_g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame(compute_variation, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a column containing the daily return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily return\n",
    "def add_daily_return(df):\n",
    "\treturn df.withColumn('Daily return', col('Open') - col('Close'))\n",
    "\n",
    "df_dr = add_daily_return(df)\n",
    "df_dr.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max daily return :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest daily return\n",
    "def get_max_daily_return(df):\n",
    "\tdf = add_daily_return(df)\n",
    "\treturn df.select(max('Daily return')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_daily_return = get_max_daily_return(df)\n",
    "df_dr.filter(col('Daily return') == max_daily_return).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation of the average daily return for different time periods :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_daily_return(df, gran, var):\n",
    "    df = add_daily_return(df)\n",
    "    return compute_average(df, gran, var)\n",
    "\n",
    "display_frame(get_average_daily_return, df, (mean('Open').alias('Open_Mean'), mean('Close').alias('Close_mean'), mean('Daily return').alias('Average daily return')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these values, to better visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_frame(df):\n",
    "\tcolors = {\n",
    "\t\t'APPLE': 'r',\n",
    "\t\t'AMAZON': 'b',\n",
    "\t\t'FACEBOOK': 'c',\n",
    "\t\t'GOOGLE': 'y',\n",
    "\t\t'MICROSOFT': 'm',\n",
    "\t\t'TESLA': 'k',\n",
    "\t\t'ZOOM': 'g'\n",
    "\t}\n",
    "\n",
    "\tfig, axs = plt.subplots(3, 3)\n",
    "\n",
    "\tj = 0\n",
    "\tfor gran in ['year', 'week', 'day']:\n",
    "\t\tdf_tmp = compute_average(df, gran, (mean('Daily return').alias('Daily return Mean'),\n",
    "\t\t\t\t\t\t\t\t\t\t\tmean('Open').alias('Open Mean'), \n",
    "\t\t\t\t\t\t\t\t\t\t\tmean('Close').alias('Close mean'), \n",
    "\t\t\t\t\t\t\t\t\t\t\tmin('Date').alias('Date')))\n",
    "\t\tmin_date = df_tmp.select(min('Date')).collect()[0][0]\n",
    "\t\tmax_date = df_tmp.select(max('Date')).collect()[0][0]\n",
    "\t\t\n",
    "\t\ti = 0\n",
    "\t\tfor cur_graph in ['Close', 'Open', 'Daily return']:\n",
    "\n",
    "\t\t\tfor name, color in colors.items():\n",
    "\t\t\t\ttmp_col = (df_tmp\n",
    "\t\t\t\t\t\t.orderBy('Date')\n",
    "\t\t\t\t\t\t.filter(df_tmp.company_name == name))\n",
    "\t\t\t\taxs[i, j].plot(tmp_col.select('Date').collect(),\n",
    "\t\t\t\t\t\t\ttmp_col.select(cur_graph + ' Mean').collect(),\n",
    "\t\t\t\t\t\t\tcolor=color,\n",
    "\t\t\t\t\t\t\tlabel=name)\n",
    "\n",
    "\t\t\taxs[i, j].set_xlim((min_date, max_date))\n",
    "\t\t\taxs[i, j].set_title(f'{cur_graph} of stocks per {gran}')\n",
    "\n",
    "\t\t\ti += 1\n",
    "\n",
    "\t\tj += 1\n",
    "\n",
    "\taxs[0, 0].legend()\n",
    "\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_frame(df_dr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good metric is the moving average, that better show global tendancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(dataframe, column_name, nb_point):\n",
    "    def days(i): \n",
    "        return i * 86400\n",
    "    w = Window().partitionBy(\"company_name\").orderBy(col(\"Date\").cast('long')).rangeBetween(-days(nb_point), 0)\n",
    "\n",
    "    return dataframe.withColumn('Moving average', avg(column_name).over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(df, 'Close', 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the coefficient of correlations between columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "\n",
    "def correlation(data_frame_A, data_frame_B, col_name):\n",
    "    \"\"\"\n",
    "        data_frame_A: has one column Date to join the data\n",
    "        data_frame_B: has one column Date to join the data\n",
    "        col_name: must be present in both data frame\n",
    "    \"\"\"\n",
    "    return data_frame_A.join(data_frame_B.withColumnRenamed(col_name, col_name + \"_B\"), on=\"Date\",how=\"inner\").corr(col_name, col_name + \"_B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cor(df):\n",
    "    w_output = widgets.Output()\n",
    "    w_inputs_A = widgets.Dropdown(\n",
    "        options=[(companies[i], i) for i in range(len(companies))],\n",
    "        value=0,\n",
    "        description=\"Company A\",\n",
    "    )\n",
    "    w_inputs_B = widgets.Dropdown(\n",
    "        options=[(companies[i], i) for i in range(len(companies))],\n",
    "        value=0,\n",
    "        description=\"Company B\",\n",
    "    )\n",
    "    \n",
    "    w_inputs_column = widgets.Dropdown(\n",
    "        options=[(columns[i], i) for i in range(len(columns))],\n",
    "        value=0,\n",
    "        description=\"Column\",\n",
    "    )\n",
    "\n",
    "    all_w_inputs = [w_inputs_A, w_inputs_B ,w_inputs_column]\n",
    "\n",
    "    def cor_display(change):\n",
    "        w_output.clear_output()\n",
    "        with w_output:\n",
    "            comp_A = w_inputs_A.options[w_inputs_A.value][0]\n",
    "            company_A_df = df.filter(df.company_name == comp_A)\n",
    "            \n",
    "            comp_B = w_inputs_B.options[w_inputs_B.value][0]\n",
    "            company_B_df = df.filter(df.company_name == comp_B)\n",
    "            \n",
    "            column = w_inputs_column.options[w_inputs_column.value][0]\n",
    "            \n",
    "            print(correlation(company_A_df, company_B_df, column))\n",
    "\n",
    "    for w in all_w_inputs:\n",
    "        w.observe(cor_display)\n",
    "        display(w)\n",
    "    display(w_output)\n",
    "\n",
    "display_cor(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot correlation matrices for better visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_corr_for_col(df, col_name, companies):\n",
    "\n",
    "    corr = [None] * len(companies)\n",
    "    for i, comp in enumerate(companies):\n",
    "        corr[i] = [0.0] * len(companies)\n",
    "        for j in range(i):\n",
    "            corr[i][j] = corr[j][i]\n",
    "        for j, other in enumerate(companies[i:]):\n",
    "            corr[i][i + j] = correlation(df.filter(df.company_name == comp), df.filter(df.company_name == other), col_name)\n",
    "    \n",
    "    return spark.createDataFrame(\n",
    "        corr, ','.join([f\"{c} float\" for c in companies])\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def display_corr_for_cols(df, cols, companies, uniform_bar=False):\n",
    "    w = 3\n",
    "    h =  len(cols) // 3 + (0 if len(cols) % 3 == 0 else 1)\n",
    "    fig, axs = plt.subplots(h, w, figsize=(22, h * 4))\n",
    "    \n",
    "    for i, col in enumerate(cols):\n",
    "        df_corr = compute_corr_for_col(df, col, companies)\n",
    "        ax = axs[i // 3, i % 3] if len(cols) > 3 else axs[i]\n",
    "        mat = ax.matshow(df_corr.collect())\n",
    "        plt.sca(ax)\n",
    "        plt.xticks(range(len(companies)), companies, fontsize=14, rotation=45)\n",
    "        plt.yticks(range(len(companies)), companies, fontsize=14)\n",
    "        cb = fig.colorbar(mat, ax=ax,fraction=0.05, pad=0.08)\n",
    "        if uniform_bar:\n",
    "            mat.set_clim(-1, 1)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.title.set_text(f\"Correlation Matrix for column {col}\");\n",
    "    \n",
    "    for j in range(len(cols), w * h):\n",
    "        ax = axs[j // 3, j % 3] if len(cols) > 3 else axs[j]\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=1, \n",
    "                    hspace=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_corr_for_cols(df, ['Volume', 'Low', 'High', 'Open', 'Close', 'Adj Close'], companies)\n",
    "\n",
    "display_corr_for_cols(df, ['Volume', 'Low', 'High', 'Open', 'Close', 'Adj Close'], companies, uniform_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the return rate, which indicates the net gain or loss of an investment over a specified time period, expressed as a percentage of the investment’s initial cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_return_rate(cur_df, gran, var):\n",
    "    cur_df = compute_variation(cur_df, gran, var)\n",
    "    cur_df = cur_df.withColumn(\"Return_rate\", when(isnull(cur_df.Close_diff / cur_df.Close_mean * 100), 0)\n",
    "                            .otherwise(cur_df.Close_diff / cur_df.Close_mean * 100))\n",
    "    return cur_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frame(compute_return_rate, df, (mean('Open').alias('Open_Mean'), mean('Close').alias('Close_mean'), mean('Volume').alias('Volume_mean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also return the company with the best return rate for a given month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "def best_return_rate(data_frame, initial_date_str, gran):\n",
    "    if gran == \"month\":\n",
    "        period_size = relativedelta(months=1)\n",
    "        date = datetime.strptime(initial_date_str, '%Y-%m').date()\n",
    "    elif gran == \"year\":\n",
    "        period_size = relativedelta(years=1)\n",
    "        date = datetime.strptime(initial_date_str, '%Y').date()\n",
    "    else:\n",
    "        print('Bad granularity, accepted are : month or year')\n",
    "        return\n",
    "    \n",
    "    initial_date = date\n",
    "    end_date = date + period_size\n",
    "    \n",
    "    df = compute_return_rate(data_frame, gran, (mean('Open').alias('Open_Mean'), mean('Close').alias('Close_mean'), mean('Volume').alias('Volume_mean'), min('Date').alias('Date')))\n",
    "    \n",
    "    df = df.filter(df.Date >= initial_date)\n",
    "    df = df.filter(df.Date < end_date)\n",
    "    \n",
    "    df = df.select('Date', 'company_name', 'Return_rate')\n",
    "    \n",
    "    max_rr = df.select(max('Return_rate')).collect()[0][0]\n",
    "    df = df.filter(df.Return_rate == max_rr) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_return_rate(df, '2018', 'year').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some insights that we can implement :\n",
    "\n",
    "- stability (différence between 'High' and 'Low')\n",
    "- add median\n",
    "- add standart deviation and interquartile difference\n",
    "- stocks prediction with time series prediciton model (RNN)\n",
    "- lowest daily return\n",
    "- lowest return rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRegDF(df, company, gran=\"day\", max_lag=5):\n",
    "    data_cols = ['Close', 'Open', 'High', 'Low']\n",
    "    cols = (min('Date').alias('Date'), mean('Close').alias('Close'), mean('Open').alias('Open'), mean('High').alias('High'), mean('Low').alias('Low'))\n",
    "    \n",
    "    df_base = df.filter(df.company_name == company)\n",
    "\n",
    "    df_lr = compute_average(df_base, gran, cols)\n",
    "    my_window = Window.orderBy(order_by[gran])\n",
    "\n",
    "    inputCols = []\n",
    "    for i in range(1, max_lag):\n",
    "        for c in data_cols :\n",
    "            col_name = f\"{c} {gran}-{i}\"\n",
    "            df_lr = df_lr.withColumn(col_name, lag(df_lr[c], i).over(my_window))\n",
    "            inputCols.append(col_name)\n",
    "\n",
    "    df_lr = df_lr.dropna()\n",
    "    return df_lr, inputCols\n",
    "\n",
    "df_lr, inputCols = buildRegDF(df, \"TESLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression, IsotonicRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "def buildRegPipelineModel(df_lr, colToEstimate, inputCols):\n",
    "    trainDF, testDF = df_lr.randomSplit([.8, .2], seed=42)\n",
    "    print(f\"There are {trainDF.cache().count()} rows in the training set, and {testDF.cache().count()} in the test set\")\n",
    "\n",
    "    vecAssembler = VectorAssembler(inputCols=inputCols, outputCol=\"features\")\n",
    "\n",
    "    reg = LinearRegression(featuresCol=\"features\", labelCol=colToEstimate) \n",
    "    # reg = IsotonicRegression(featuresCol=\"features\", labelCol=colToEstimate)\n",
    "\n",
    "    pipeline = Pipeline(stages=[vecAssembler, reg])\n",
    "    pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "    predTrainDF = pipelineModel.transform(trainDF)\n",
    "    predTestDF = pipelineModel.transform(testDF)\n",
    "\n",
    "    # predTrainDF.select(colToEstimate, \"prediction\").show(10)\n",
    "    # predTestDF.select(colToEstimate, \"prediction\").show(10)\n",
    "\n",
    "    regressionMeanEvaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=colToEstimate, metricName=\"rmse\")\n",
    "\n",
    "    print(f\"The RMSE for predicting the {colToEstimate} (train) is: {regressionMeanEvaluator.evaluate(predTrainDF):.2f}\")\n",
    "    print(f\"The RMSE for predicting the {colToEstimate} (test) is: {regressionMeanEvaluator.evaluate(predTestDF):.2f}\")\n",
    "\n",
    "    return pipelineModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shiftColumn(df, inputCols, cols_name):\n",
    "    cols_shift = cols_name + inputCols\n",
    "    for i in range(len(cols_shift) - 1, len(cols_name) - 1, -1):\n",
    "        df = df.withColumn(cols_shift[i], col(cols_shift[i - 4]))\n",
    "        # print(f\"Shift from {cols_shift[i - 4]} to {cols_shift[i]}\")\n",
    "    return df\n",
    "\n",
    "def predictCaracteristic(df, company, offset, gran=\"day\", max_lag=2):\n",
    "    data_cols = ['Close', 'Open', 'High', 'Low']\n",
    "\n",
    "    df_lr, inputCols = buildRegDF(df, company, gran, max_lag)\n",
    "\n",
    "    mDate = df_lr.select(max('Date')).collect()[0][0]\n",
    "    last_line = df_lr.orderBy(col('Date').asc()).filter(df_lr.Date == mDate)\n",
    "\n",
    "    pipelines = [buildRegPipelineModel(df_lr, col, inputCols) for col in data_cols]\n",
    "\n",
    "    for i in range(offset):\n",
    "        print(f\"{gran} +{i}\")\n",
    "        # last_line.show()\n",
    "        # last_line.select(data_cols + inputCols).show()\n",
    "        # last_line.select(data_cols).show()\n",
    "\n",
    "        last_line = shiftColumn(last_line, inputCols, data_cols)\n",
    "        \n",
    "        for ch in range(len(data_cols)):\n",
    "            last_line = pipelines[ch].transform(last_line)\n",
    "            last_line = last_line.withColumn(data_cols[ch], last_line.prediction).drop(\"prediction\", \"features\")\n",
    "            \n",
    "    return last_line\n",
    "    \n",
    "predictCaracteristic(df, \"TESLA\", 10, max_lag=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d9e8179dbc941d97ff6d11e1120d317cbcac3b49646012c4d379b8a332df6437"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
